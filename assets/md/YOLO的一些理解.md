# YOLO的一些特点
1. 单阶段（V1）
2. 划分SXS格机制（V1）
3. Anchor Box 机制（V2）
4.  Darknet-19（V2）
5.  细粒度特征（Fine-Grained Features）V2
6.  darknet-53 V3
7.  多尺度预测  V3
8.  损失函数改进  V3
9.  多标签分类 V3
10. V4架构
11. V4技巧
12. V5改进
13. Anchor-Free的思想 V8
## 单阶段
1. 把目标检测转变成一个回归问题，利用整张图作为网络的输入，仅仅经过一个神经网络，得到bounding box（边界框） 的位置及其所属的类别。
2. 双阶段：R-CNN，精度高但速度慢；生成候选区域+分类，精修位置
## 划分SXS格机制（分而治之V1）
1. 通过 “每个格子只能预测有限个目标 + 中心点唯一归属” 来解决重复标注问题。
2. 空间分区 + 预测数量上限”来控制冗余
3. 早期的滑动窗口、或者某些密集预测模型会在每一个可能的位置都尝试预测，生成数百的候选框，给NMS带来极大压力。
### 实现
1. 将一幅图像分成 S×S个网格（v1是7X7），如果某个 object 的中心落在这个网格中，则这个网格就负责预测这个object。
2. 每个网格要预测 B 个bounding box（代表不同形状的框以适应不同目标的宽高，v1是2个），每个 bounding box 要预测 (x, y, w, h) 和 confidence 共5个值。
3. 每个网格还要预测一个类别信息，记为 C 个类。
4. 总的来说，S×S 个网格，每个网格要预测 B个bounding box ，还要预测 C 个类。网络输出就是一个 S × S × (5×B+C) 的张量。
### 损失函数
1. 定位损失：Bounding box 的 confidence主要是 IOU 的估计值（定位质量）。而不是类别概率。
2. 如果有物体，它是什么？ → 由类别概率回答；C = 3：[猫, 狗, 车]分别代表目标是对应物体的概率。
### v1缺点
1. 对相互靠近的物体，以及很小的群体检测效果不好，这是因为一个网格只预测了2个框，并且都只属于同一类。
2. 由于损失函数的问题，定位误差是影响检测效果的主要原因，尤其是大小物体的处理上，还有待加强。（因为对于小的bounding boxes，small error影响更大）
3. YOLO对不常见的角度的目标泛化性能偏弱。
## Anchor Box 机制（v2）
### 要解决的问题
1. 尺度敏感：预测一个 10×10 的小框 vs 300×300 的大框，误差尺度完全不同+网络很难同时学好大小物体
2. 形状多样：单靠固定数个框很难适应所有形状
### 实现
核心思想：“先猜个大概，再微调”
1. 在训练前，通过聚类分析（如 K-Means）统计训练集中所有真实框的 宽高比（aspect ratio）和尺度（scale），选出最常见的 B 种形状，作为 Anchor Boxes
2. 每个网格对应 B 个 Anchor
3. 模型不再预测 (w, h)，而是预测“偏移量”;
对每个 Anchor，模型输出：
- $t_x, t_y$：中心点相对于 Anchor 中心的偏移
- $t_w, t_h$：宽高相对于 Anchor 宽高的缩放因子

然后通过公式还原真实框：

$$
\begin{aligned}
b_x &= \sigma(t_x) + c_x \\
b_y &= \sigma(t_y) + c_y \\
b_w &= p_w \cdot e^{t_w} \\
b_h &= p_h \cdot e^{t_h}
\end{aligned}
$$

其中：
- $(c_x, c_y)$：当前网格/位置的左上角坐标（归一化）
- $(p_w, p_h)$：Anchor 的宽和高（归一化）
- $\sigma$：sigmoid 函数，保证中心点落在当前格子内

> ✅ 这样，网络只需要学习**小的调整量**（比如 $t_w = 0.1$ 表示“比 Anchor 宽 10%”），而不是从零预测绝对尺寸！
##  Darknet-19
### 架构
1. 与VGG相似，使用了很多3×3卷积核；并且每一次池化后，下一层的卷积核的通道数 = 池化输出的通道 × 2。
2. 在每一层卷积后，都增加了批量标准化（Batch Normalization）进行预处理。
3. 采用了降维的思想，把1×1的卷积置于3×3之间，用来压缩特征。
4. 在网络最后的输出增加了一个global average pooling层。
5. 整体上采用了19个卷积层，5个池化层。
### 技巧
1. 1×1 卷积降维（Network in Network 思想）
- 卷积：（64X64X3X1）的图像（宽高通道个数）与（3X3X3X10）的卷积核（10个3X3X3的不同卷积核）得到（62X62X1X10）的特征图（宽高通道个数）->（3X3X10X10）的卷积核得到（60X60X1X10）
- 这里的降维就是（1x1x10X8）将10个特征图降为8个。
- 实质（宽X高X通道X输出特征图个数）也可以将输出特征图个数看作一个图的不同通道。
2. 用卷积代替全连接层（Fully Convolutional）
- 整个网络变成 全卷积网络（FCN）
- 支持任意尺寸输入（只要能被 32 整除）
- 参数大幅减少（全连接层参数最多！ 局部连接 + 权重共享实现）
- 更适合滑动窗口式检测
3. Batch Normalization（批归一化） everywhere
- 每个卷积层后都加 BN + Leaky ReLU
- 加速训练收敛，减少对 dropout 的依赖（Darknet-19 不用 dropout！），提升模型泛化能力
- 实现：训练阶段计算批均值和方差->归一化 -> 缩放和平移（可学习参数）；推理阶段使用移动平均。
- 计算: C 个独立的均值和方差（每个通道一个）
- 在卷积或池化之后，激活函数之前（激活函数会改变数据分布）
4. 高分辨率分类器预训练（High-Resolution Classifier）
先在低分辨率稳定收敛，再微调高分辨率
5. 多尺度训练（Multi-Scale Training）
训练中每 10 batch 随机切换输入尺寸（320~608）
## 细粒度特征
应对问题：小物体检测时被“池化”或“下采样”为非常小的像素。
### 实现（Passthrough Layer（直通层））
1. 从浅层提取高分辨率特征
2. 将高分辨率特征“压缩”后拼接到深层
把 26×26×512 的特征图，通过 reorg 操作（多行矩阵变单行）变成：（26×26×512->13x13x4x512->13x13x2048）
3. 然后把这个 2048 通道的细粒度特征 与原来的 13×13×1024 深层特征 拼接 -> 13×13×(1024+2048)
## darknet-53
见Darknet-53架构及技巧.md
## 多尺度预测
见Darknet-53架构及技巧.md 多尺度特征提取部分
## 损失函数改进
位置损失部分并没有改变，但是置信度损失和类别预测均由原来的平方和误差改为了交叉熵的损失计算方法。对于类别以及置信度的预测，使用交叉熵的效果应该更好。
> 交叉熵损失:越不可能发生的事，一旦发生，带来的‘惊讶程度’就越高。
> 交叉熵损失 = -log(模型对真实类别的预测概率)
> 预测概率越接近 0，损失爆炸式增长！能惩罚“自信的错误”。
> 优势：梯度友好；最大似然估计的等价形式；天然惩罚“高置信度错误”；
## 多标签分类
解决的问题：在一些复杂的场景中，单一目标可能从属于多个类别。
### 实现：对每个类别独立做二分类
1. 输出层：使用 Sigmoid 激活函数（不是 Softmax）
- 每个神经元输出 ∈ (0, 1)，表示“属于该类的概率”
- 各类别相互独立
2. 损失函数：使用 Binary Cross-Entropy（BCE二元交叉熵） 对每个类别分别计算，再求平均（或求和）
> 二元交叉熵:BCE = -[ y·log(p) + (1−y)·log(1−p) ],真实标签 y(0,1),预测p（0~1）。
##  V4架构
YOLOv4 = CSPDarknet53（主干） + SPP附加模块（颈） + PANet路径聚合（颈） + YOLOv3（头部）
### CSPDarknet53（分而治之 + 融合 跨阶段局部网络）
核心思想：将输入特征图分两路处理，一路直接 bypass，一路经过残差块，最后融合。
#### 实现
1. 将输入通道 split 成两部分（通常 1:1）
> Part A（50% 通道）→ 直接 bypass(直接传递)
> Part B（50% 通道）→ 送入残差块序列
2. Part B 经过 N 个残差块 → 得到 processed feature
3. 将 Part A 和 processed Part B 拼接（concatenate）
4. 再通过一个 transition layer（1×1 卷积）融合
#### 技巧
1. Mish 激活函数（平滑、非单调、提升精度（但稍慢））
2. 全卷积 + 步长卷积下采样（无池化）
3.  多尺度输出 + PANet 融合（在检测头中）
4.  Batch Normalization + 自适应激活
### SPP附加模块（Spatial Pyramid Pooling，空间金字塔池化）
#### 定义
SPP 模块通过在不同尺度上做池化，然后拼接结果，使网络能捕捉多尺度上下文信息，从而提升对不同大小物体的鲁棒性。
#### 核心思想：多尺度池化 + 特征融合
#### 解决的问题：
固定感受野有局限性，SPP使模型既知道细节，又知道整体场景。
#### 实现
1. 所有池化使用 stride=1 + padding，保持 spatial size 不变（如 13×13）
2. 使用 不同 kernel size（如 5, 9, 13）模拟多尺度上下文
3. 最后与原始特征拼接 → 融合局部 + 多尺度全局信息
>  kernel size 是经验选择的，覆盖了从局部到接近全局的范围。
### PANet（Path Aggregation Network，路径聚合网络）
#### 一句话总结
PANet 通过构建“自上而下 + 自下而上”的双向特征金字塔，让高层语义信息和底层定位信息充分流动，从而显著提升对小物体、大物体和遮挡物体的检测/分割性能。
#### 解决的问题
FPN（特征融合模块）只能将高层语义传到底层，帮助小物体分类，而底层的精确定位信息无法有效传递到高层！
#### 核心思想：双向特征金字塔
#### 实现
假设 backbone 输出三层特征：C3（52×52）、C4（26×26）、C5（13×13）
1. FPN 路径（自上而下）
- P5 = C5
- P4 = Upsample(P5) + C4
- P3 = Upsample(P4) + C3
→ 得到 {P3, P4, P5}，语义逐层增强
2. PANet 新增路径（自下而上）
- N3 = P3
- N4 = Downsample(N3) + P4
- N5 = Downsample(N4) + P5
→ 得到 {N3, N4, N5}，定位信息逐层增强
3. 输出
- 使用 {N3, N4, N5} 作为检测头的输入
- 每一层都同时包含强语义 + 精确定位
## V4技巧
### 免费包
在不增加推理成本的前提下获得更好的精度，而只改变训练策略或只增加训练成本的方法。
#### 数据增强方法
- 随机缩放
- 翻转、旋转
- 图像扰动、加噪声、遮挡
- 改变亮度、对比对、饱和度、色调
- 随机裁剪（random crop）（从原始图像中随机截取一个子区域（通常保持宽高比或固定尺寸）然后 resize 到模型输入尺寸（如 224×224））
- 随机擦除（random erase）：随机选择一个矩形区域用随机值（或均值/0）填充该区域
- Cutout：随机擦除固定用 0（黑色） 填充
- MixUp：线性插值两个样本及其标签，
- CutMix：用一张图的一部分“替换”另一张图的对应区域同时按面积比例混合标签

| 场景 | 推荐方法 |
|------|--------|
| **小数据集分类** | Random Erase + CutMix |
| **大模型防过拟合** | MixUp 或 CutMix |
| **目标检测训练** | Random Crop + Mosaic + HSV 色彩扰动 |
| **提升遮挡鲁棒性** | Cutout / Random Erase（分类）或 Copy-Paste（检测） |

#### 正则化方法
- DropOut:在全连接层（FC） 中，以概率 p 随机将某些神经元的输出置为 0,测试时：所有神经元保留，但输出乘以 (1−p)（或训练时除以 (1−p)，即 inverted dropout）
- DropConnect:不是丢弃神经元输出，而是丢弃权重连接,对权重矩阵 W 中的每个元素，以概率 p 置为 0.
- DropBlock:在卷积特征图上，随机选择若干个 block（正方形区域）,将这些 block 内的所有通道全部置 0,block 大小（如 7×7）、丢弃率 γ 可调
#### 平衡正负样本的方法
- Focal loss
- OHEM(在线难分样本挖掘)
#### 回归 损失方面的改进
- GIOU
- DIOU
- CIoU
### 特价包
只增加少量推理成本但能显著提高目标检测精度的插件模块和后处理方法。
#### 增大感受野技巧
- SPP
- ASPP
- RFB
#### 注意力机制
- Squeeze-and-Excitation (SE)
- Spatial Attention Module (SAM)
#### 特征融合集成
- FPN
- SFAM
- ASFF
- BiFPN （出自于大名鼎鼎的EfficientDet）
#### 更好的激活函数
- ReLU
- LReLU
- PReLU
- ReLU6
- SELU
- Swish
- hard-Swish
#### 后处理非极大值抑制算法
- soft-NMS
- DIoU NMS
#### 适应在单GPU上训练
1. Mosaic
借鉴了CutMix数据增强方式的思想。CutMix数据增强方式利用两张图片进行拼接，但是Mosaic使利用四张图片进行拼接。
2. SAT
是一种自对抗训练数据增强方法，通过梯度上升，修改原始图像，使其最大化当前模型的损失，得到一张“人眼看不出区别，但模型会判错”的对抗图像，正常更新模型参数，最小化在对抗图像上的损失，让模型学会正确识别这种“最坏情况”。
3. CmBN
跨小批量归一化（Cross mini-Batch Normalization），在一次前向传播中，把 batch 分成 K 个子块（sub-batches），然后在反向传播前，累积这 K 次的统计量。单GPU显存不足以使用大batch，会因显存不足出现统计错误的情况，但分开+累计可以实现大batch效果。
4. 修改过的SAM
SAM（Spatial Attention Module） 来自《CBAM: Convolutional Block Attention Module》，结构如下：
输入 feature map → 分别做 通道平均池化 & 最大池化 → 拼接 → 卷积 → sigmoid → 生成 spatial mask
用 mask 乘原 feature → 强调重要区域；
- 修改点：
  1. 去掉通道池化，直接对 feature map 做卷积
  2. 使用 单个卷积层（kernel=1 或 3） 生成 attention mask
  3. mask 应用于残差连接之后（而非替换原特征）
  4. 更轻量，避免性能下降
5. 修改过的PAN
主要修改：
- 将特征融合方式从 “相加（add）” 改为 “拼接（concatenate）”
  - Concat 保留更多信息（add 会丢失部分通道信息）
  - 后续用 1×1 卷积降维
- 在 PAN 路径中加入 CSP 结构
  - 减少计算冗余
  - 提升梯度流效率
- 配合 SPP 模块使用
  - 在 PAN 输入前先经过 SPP，增强多尺度上下文
## V5改进
### 自适应锚框计算
通过K-Means方法来获取数据集的最佳anchors改为此功能嵌入到整体代码中，每次训练时，自适应的计算不同训练集中的最佳锚框值。
### 自适应灰度填充
应对输入图片尺寸 不一的问题，使用灰度填充，其核心思想就是将原图的长宽等比缩放对应统一尺寸，然后对于空白部分用灰色填充。
### 损失函数
- 分类用交叉熵损失函数（BEC Loss），边界框回归用 CIoU Loss。
#### CIOU
重叠（IoU）+ 位置（中心距离）+ 形状（宽高比）
##### 公式（上标gt表示真实值）
$$
\text{CIoU} = \text{IoU} - \frac{\rho^2(b, b^{gt})}{c^2} - \alpha v
$$

其中：
- $ \rho^2(b, b^{gt}) $：中心点距离平方
- $ c $：最小包围盒对角线长度
- $ v $：**宽高比一致性度量**
- $ \alpha $：权重系数（自适应调整）
##### 1. 宽高比差异 $ v $：
$$
v = \frac{4}{\pi^2} \left( \arctan\frac{w^{gt}}{h^{gt}} - \arctan\frac{w}{h} \right)^2
$$
- 衡量预测框与真实框的**宽高比差异**
- 值 ∈ [0, 1]，越接近 0 表示宽高比越一致
- arctan把宽高比（形状信息）映射到角度空间，对称性更好且arctan在 [0, ∞) 上更平滑，且天然有界，优化更稳定。
- $\frac{4}{\pi^2}$将角度归一化到 [0,1]。
##### 2. 权重 $ \alpha $（自适应）：
$$
\alpha = \frac{v}{(1 - \text{IoU}) + v}
$$
- 当 IoU 较大（框已较准）时，$ \alpha \to 1 $，**强调宽高比对齐**
- 当 IoU 很小（框很偏）时，$ \alpha \to 0 $，**优先优化位置和重叠**

> 💡 这种自适应机制非常聪明：**先定位，再调形状！**
## Anchor-Free的思想
### 总结
不再预设任何锚框（anchor boxes），而是让网络在特征图的每个位置直接预测：
“这里是否存在目标？如果有，它的边界框中心偏移、宽高（或四边距离）是多少？”
### 解决的问题（Anchor-Based 的三大痛点）
1. 依赖数据集先验（ k-means 聚类）
2.  超参数敏感（额外的参数）
3.  正样本分配复杂（一个真实框（GT）可能匹配多个 anchor）
### 实现
1. 每个 grid cell（SXS的格子） 只预测一个 bbox
2. 预测内容：直接回归边界信息
   - 模型预测 从 grid cell 中心点到 bbox 四条边的距离（left, top, right, bottom）
   -  采用 DFL（Distribution Focal Loss） + 边界距离回归 的方式
3. 坐标解码：基于 grid cell 中心点
4.  配合 TAL（Task-Aligned Assigner）动态分配正样本  
    - 不再基于 IoU 与 anchor 匹配，而是根据预测质量动态选择正样本：  
### 对实现的补充
#### 损失函数改进（包含实现中的DFL）
##### VFL Loss作为分类损失
###### 介绍
将 IoU 作为分类分数的直接监督目标，原本的分类与标框是分开的，可能出现分类低（但正确）但标框好的情况，导致相乘得到置信度被拉低，进而被NMS滤掉。改进后V8的置信度就只输出IACS了。
VFL 提出两个关键创新：
###### ✅ 1. **IoU-aware Classification Score（IACS）**
- 不再让分类分支只预测“类别概率”
- 而是预测一个融合了 **分类置信度 + 定位精度** 的分数：
  $$
  \text{IACS} = P(\text{class}) \times \text{IoU}
  $$
- 这个分数直接用于 NMS 排序（比单纯用分类分更合理）

###### ✅ 2. **不对称焦点损失（Asymmetric Focal Loss）**
- 对**正样本**和**负样本**使用**不同的聚焦因子**
- 公式如下：

$$
\text{VFL}(p, q) = 
\begin{cases}
-q \cdot \log(p), & \text{if } y = 1 \quad \text{(正样本)} \\
-\alpha \cdot p^\gamma \cdot \log(1 - p), & \text{if } y = 0 \quad \text{(负样本)}
\end{cases}
$$

其中：
- $ p $：模型预测的 IACS 分数（∈ [0,1]）
- $ q $：**目标分数**（不是 1！而是 **真实 IoU 值**）
- $ \alpha, \gamma $：超参数（通常 α=0.75, γ=2）

> 🌟 **最关键的区别**：  
> **正样本的标签不是 1，而是真实的 IoU！**
##### DFL Loss+CIOU Loss作为边界框回归损失
###### DFL 的做法：用分类模拟回归
1. 将连续距离离散化
    1. 设定一个范围，比如 [0, 16) 像素（可缩放）
    2. 将其划分为 16 个区间（bins），每个 bin 宽度 = 1
    3. 真实距离 d=5.3 → 落在第 5 和第 6 个 bin 之间 
2. 模型输出一个概率分布
    1. 对每个边界（l/t/r/b），模型输出 17 个 logits（对应 16 个 bin 的 17 个端点）
    2. 通过 softmax 得到概率分布$p_0$~$p_{16}$
3. 用分布计算期望值（即预测距离）
    - 实际实现中，直接用 加权和$p_i$*$i$求和

###### Distribution Focal Loss（DFL Loss）
设真实距离 $ d = 5.3 $
- 左侧整数：$ l = \lfloor d \rfloor = 5 $
- 右侧整数：$ r = l + 1 = 6 $
- 权重：$ w_l = r - d = 0.7 $，$ w_r = d - l = 0.3 $

Loss 定义为：
$$
\mathcal{L}_{\text{DFL}} = - \left[ w_l \cdot \log(p_l) + w_r \cdot \log(p_r) \right]
$$

> ✅ **只监督最近的两个位置**，让模型学会“插值”，从而精确表示 5.3！
> 学习的就是前面DFL的输出概率分布。
#### 样本匹配（Task-Aligned Assigner匹配方式）
✅ TAL 的核心思想：**根据预测质量动态分配**

> “谁预测得好，谁就负责这个目标。”
##### 实现
1. 对每个 GT，计算它与**所有 grid 预测框**的对齐分数 $ s $
2. 选出 **top-k 个最高分的 grid** 作为正样本（k=10）
3. 同时，**过滤掉 cls_score < 阈值（如 0.5）的低质量预测**
###### 对齐分数（Alignment Score）
$$
s = \text{cls\_score}^\alpha \times \text{IoU}^\beta
$$
- `cls_score`：该 grid 对该类别的预测置信度
- `IoU`：该 grid 预测框与 GT 的 IoU
- α, β：超参数（YOLOv8 默认 α=1.0, β=6.0）

> 💡 **IoU 权重更高** → 强调定位准确性
###### 目标检测中的正样本
正样本（Positive Sample） = 应该负责预测某个真实目标（GT）的预测位置（grid）
负样本（Negative Sample） = 不负责任何 GT 的预测位置
正样本用于计算Loss
