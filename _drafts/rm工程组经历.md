---
layout: post
title: RM工程组经历
date: 2026-03-01 15:02 +0800
description: '工程组视觉复盘：从6D位姿方案复刻失败到传统视觉落地'
categories: [复盘,项目复盘]
tags: [RoboMaster, 工程组, 视觉]
---

## 背景与目标

这段经历发生在 RoboMaster 工程组赛季内，我负责工程组视觉方向的主要工作，目标是在有限时间内给机械臂提供**可用的目标检测 + 位姿解算**能力，覆盖：

- 矿石识别与位姿解算
- 兑换框（正面框 + 侧面箭头）的识别与位姿解算
- 与相机、串口、部署环境的对接（与嵌入式协作）

当时团队整体约束比较典型：成员技能不完全匹配工程难度、硬件/环境约束多、赛程时间不可逆。复盘的重点是我在**方案选择、问题定位、止损与交付**上的得失。

## 学习与能力基线（项目开始时）

我在项目开始前/期间的学习大多是“能用但不系统”的状态：

- ROS2：跟着官方文档学到 URDF，后续未深入
- C++：通过 B 站教程学到类等基础；后续补完但知识点碎片化
- CMake / Git / Linux：从 CSDN、知乎等学习基础，会用但依赖 AI 辅助查用法
- 图像处理 / OpenCV：跟队长 3 小时快速入门；后续用到时边做边查资料与 AI
- 视觉 SLAM：学到非线性优化前几章，后续未深入，赛季后基本遗忘
- YOLO / OpenVINO：会做数据集、标注、训练、部署；偏应用层，不熟悉原理
- 相机标定：会操作流程，原理掌握不足

这意味着：我适合做“工程落地/应用组合”，但在需要深厚理论与大量工具链协作的方向上（例如 6D 位姿端到端方案），风险会更高。

## 职责与交付范围

我在工程组的职责聚焦在视觉链路：

- **矿石识别**：输出矿石类别与位姿（用于机械臂抓取/定位）
- **兑换框识别**：输出正面框与侧面箭头的位姿
- **工程对接**：相机驱动/数据获取、与串口通信接口适配、在 Intel NUC 上部署验证

机械臂运动规划方面，我最初尝试 MoveIt2，但因精力与时间约束，后续移交给嵌入式同学负责。

## 方案1：复刻开源“单目 RGB 6D 位姿估计”方案（失败复盘）

### 选型动机

赛季初我希望用一条“工程界已有成功案例”的路径快速达成目标，于是尝试复刻华南虎的开源工程视觉方案（当时几乎是唯一公开的工程视觉完整方案）。其核心思路是：

- 单张 RGB 图像进行 6D 位姿估计
- 数据集：Linemod 格式，包含 BlenderProc 渲染数据集制作；（并结合真实数据的点云重建与自动语义分割）
- 网络：EfficientPose 训练推理

### 实际踩坑与根因分析

1) **真实数据集质量与目标外观不稳定**

- 我用借来的深度相机完成了 Linemod 格式数据采集与制作，但训练/推理效果与开源展示差距较大。
- 事后反思：矿石表面凹凸、字符不清晰、以及可能的拍摄流程问题（光照/角度/距离/曝光/背景一致性）使得“单目姿态估计”的难度显著提高。

2) **BlenderProc 渲染代码的单位 Bug 导致长时间卡死**

- 原代码单位使用 mm 而非 m，导致渲染物体极小、加载/显示异常，我在不知道根因的情况下反复尝试，debug 花费了大量时间。
- 事后反思：这是典型的“工具链复杂 + 代码理解不足”的组合问题。我对 BlenderProc/渲染流程并不熟悉，缺少快速验证（例如打印尺度、场景单位检查、最小可复现用例）的习惯。

3) **训练环境与 CUDA 版本不兼容**

- EfficientPose 项目支持的 CUDA 版本较旧，无法在现有 GPU 环境上顺利训练。
- 我尝试替换网络，但短时间内没找到同时满足“支持 Linemod 格式 + RGB 单目姿态推理 + 可在现有环境训练”的替代方案。

4) **决策与节奏问题：没有及时止损**

- 在对 6D 姿态估计方向几乎缺乏经验的前提下，我仍持续投入时间试错，导致成本继续扩大。
- 曾考虑租用云服务器训练，但由于时间、持续出错造成的信任损耗等因素，最终未能推进。

### 阶段结论

这条路线以“方案告吹”收尾，我也因此降为梯队。

复盘这次失败，我认为最关键的问题不在于“遇到 bug”，而在于：

- **对自身能力边界判断偏乐观**：0 基础直接进入 6D 位姿端到端方案，投入产出比失控
- **缺少里程碑与止损线**：没有明确的最小可用目标（MVP）与 T+N 天必须产出什么
- **缺少工程化验证方法**：复杂链路没有拆成可独立验证的小环节（数据→渲染→训练→推理→评估）

这些问题直接影响团队节奏，是我在“项目管理意识”上的明显短板。

## 方案2：传统视觉轮廓方案快速落地（最终交付）

在 6D 网络方案失败后，为了尽快产出可用成果，我转向传统视觉方案，并参考华南虎兑换槽解算的思路设计矿石位姿解算。

### 矿石识别：HSV + 轮廓 + PnP 位姿解算

整体流程：

1. **颜色分割**：HSV 提取目标颜色区域
2. **轮廓提取**：寻找候选轮廓
3. **筛选与匹配**：基于几何特征与先验规则过滤、匹配目标
4. **后处理**：去噪、合并、稳定输出（按场景做规则修正）
5. **位姿解算**：通过 PnP 解算输出位姿给上层控制

该思路后续扩展到了银矿石、兑换框等目标：骨架一致，细节上针对不同目标的颜色/形状/遮挡情况做差异化处理。

### 兑换框识别：正面框与侧面箭头分别解算

我将兑换框分为“正面框”和“侧面箭头”两类目标分别识别与解算：

- 同样使用 HSV → 轮廓 → 筛选/匹配 → 后处理 → PnP
- 正侧切换时存在少量转换误差；结合实际场景中“基本禁止侧面视角”的约束，评估认为可接受

### 工程对接：相机与串口

- 相机包与串口包直接复用了自瞄方案的成熟实现，并做了小幅改动以适配工程组需求
- 使用迈德威视相机与 8mm 镜头，在 Intel NUC 上安装 Ubuntu 部署
- 赛前完成与嵌入式对接，但因时间原因算法未能进行充分的真实场景联调测试，存在潜在隐藏问题风险

### 计划但未落地：YOLO + 传统后处理

我原计划将识别升级为“YOLO 检测 + 传统几何/轮廓后处理 + PnP”的组合，以提升泛化与稳定性；但赛季结束较快，未能启动实现。

## 结果与产出（可用于简历表述）

在赛季后半段，我通过从高风险方案切换到可控的传统视觉链路，完成了工程组视觉方向的可用实现：

- 交付了矿石与兑换框的识别与位姿解算 pipeline（HSV/轮廓/规则/PNP），并根据不同目标做了差异化处理
- 完成相机与串口链路的工程对接，能够在 Intel NUC + Ubuntu 环境部署运行
- 将复杂任务拆成可执行的工程模块（颜色分割、轮廓筛选、位姿解算、接口适配），便于后续替换检测器（如 YOLO）

同时我也明确认识到：赛前缺少充分的实物联调与压力测试，会让“看起来能跑”与“赛场可用”之间存在不确定性。

## 关键反思：我学到了什么

### 1) 方案评估要先做 MVP 与风险清单

如果重新来过，我会把 6D 方案拆成可验证的里程碑，并设置明确止损线：

- 先验证数据集质量与标注闭环（采集→可视化→基础指标检查）
- 再验证渲染/训练环境能跑通（单位、版本、CUDA/驱动）
- 最后才投入端到端训练

只要某个环节在限定时间内无法通过，就立即转向“可交付”的备选方案。

### 2) Debug 的本质是建立“可观测性”

BlenderProc 单位问题让我意识到：面对复杂工具链，靠直觉试错会非常慢。更有效的方法是：

- 构造最小复现用例
- 逐层打印/可视化关键中间变量（尺度、坐标系、输入输出）
- 用 checklist 把隐含假设显式化（单位、坐标系、相机内参、颜色空间等）

### 3) 工程协作要提前卡接口与联调时间

视觉与嵌入式/机械臂对接是“系统工程”，应当尽早冻结接口与联调计划：

- 数据格式（时间戳、坐标系、单位）
- 串口协议与容错
- 上机验证的时间窗口（不要把联调放在赛前最后几天）

## 下一步改进方向（如果继续迭代）

- 识别升级：采用 YOLO/OpenVINO 做目标检测，提高对光照/背景变化的鲁棒性；位姿仍使用 PnP + 几何后处理
- 数据与评测：建立可复用的测试集与离线评估脚本（不同光照、距离、角度）
- 工程化：把参数（HSV 阈值、筛选条件）配置化，并增加日志与可视化工具提升可观测性
